<!--
This is for including Chapter 1.  Notice that it's also good practice to name your chunk.  This will help you debug potential issues as you knit.  The chunk above is called intro and the one below is called chapter1.  Feel free to change the name of the Rmd file as you wish, but don't forget to change it here from chap1.Rmd.
-->

<!--
The {#rmd-basics} text after the chapter declaration will allow us to link throughout the document back to the beginning of Chapter 1.  These labels will automatically be generated (if not specified) by changing the spaces to hyphens and capital letters to lowercase.  Look for the reference to this label at the beginning of Chapter 2.
-->

# An Introduction to Reproducibility {#reproducibility}

## What is reproducibility?

In the field of data science, research is considered fully *reproducible* when the requisite code and data files produce identical results when run by another analyst, or more generally, when a researcher can "duplicate the results of a prior study using the same materials as were used by the original investigator".

**Add Source:** K. Bollen, J. T. Cacioppo, R. Kaplan, J. Krosnick, J. L. Olds, Social, Behavioral, and Economic Sciences Perspectives on Robust and Reliable Science (National Science Foundation, Arlington, VA, 2015).

This term was first coined in 1992 by computer scientist Jon Claerbout, who associated it with a "software platform and set of procedures that permit the reader of a paper to see the entire processing trail from the raw data and code to figures and tables".

**Add source:** J. Claerbout, M. Karrenbach, Electronic documents give reproducible research a new meaning, in Proceedings of the 62nd Annual International Meeting of the Society of Exploration Geophysics, New Orleans, USA, 25 to 29 October 1992.

Since its inception, the concept of reproducibility has been applied across many different data-intensive fields, including epidemiology, computational biology, economics, clinical trials, and, now, the more general domain of statistical and data sciences (@Goodman341ps12).

Reproducible research has a wide variety of benefits in the scientific community. When researchers provide the code and data used for their work in a well-organized and reproducible format, readers are more easily able to determine the veracity of any findings by following the steps from raw data to conclusions. The creators of reproducible research can also more easily receive more specific feedback (including bug fixes) on their work. Moreover, others interested in the research topic can use the code to apply the methods and ideas used in one project to their own work with minimal effort. 

Although often confused, the concept of *reproducibility* is distinct from the similar idea of *replicability*: the ability of a researcher to duplicate the results of a study when following the original procedure but collecting new data. Replicability has larger-scale implications than reproducibilty; the findings of research studies can not be accepted unless a variety of other researchers come to the same conclusions through independent work.

```{r out.width = '100%'}
knitr::include_graphics("figure/versus.png")
```

Reproducibility and replicability are both necessary to the advancement of scientific research, but they vary significantly in terms of their difficulty to achieve. Reproducibility, in theory, is somewhat simple to attain in data analyses--because code is inherently non-random (excepting applications involving random number generation) and data remain consistent, variability is highly restricted. The achievement of replicability, on the other hand, is a much more complex challenge, involving significantly more variablility and requiring high quality data, effective study design, and incredibly robust hypotheses.


## The Reproducibility Crisis

Despite the relative simplicity of achieving reproducibility, a significant proportion of the work produced in the scientific community fails to meet reproducibility standards. 52% of respondents in a 2016 Nature survey believed that science was going through a "crisis" of reproducibility. Additionally, the vast majority of researchers across all fields studied reported having been unable to reproduce another researcher's results, while approximately half reported having been unable to reproduce their own. https://www.nature.com/news/1-500-scientists-lift-the-lid-on-reproducibility-1.19970 Other studies paint an even bleaker picture: a 2015 study found that over 50% of studies psychology failed reproducibility tests and research from 2012 found that figure closer to 90% in the field of cancer biology. (https://www.nature.com/news/over-half-of-psychology-studies-fail-reproducibility-test-1.18248, https://www.nature.com/articles/483531a)


In the past several years, this "crisis" of reproducibility has risen toward the forefront of scientific discussion. Without reproducibility, the scientific community cannot properly verify study results.This makes it difficult to identify which information should be believed and which should not and increases the likelihood that studies sharing misleading information will be dispersed. The rise of data-driven technologies, alongside our newly founded ability to instantly share knowledge worldwide, has made reproducibility increasingly critical to the advancement of scientific understanding, necessitating the development of solutions for addressing the issue. 

Academics have recognized this, and publications on the topic appear to have increased siginficantly in the last several years (@eisner-reproducibility; @sep-scientific-reproducibility; @bioessays-gosselin; @engineering-reproducibility; @plos-biology).

## The Components of Reproducible Research

In order to address the lack of reproducibility in scientific research, it is important to first consider the question: Which components are necessary to declare research reproducibile?

Publications attempting to answer this can be found in all areas of scientific research. However, as @Goodman341ps12 argue, the language and conceptual framework used to describe research reproducibility varies significantly across the sciences, and there are no clear standards on reproducibility agreed upon by the scientific community as a whole. 

At a minimum, according to @Goodman341ps12, documenting reproducibility requires the sharing of data (raw or processed), relevant metadata, code, and related software. However, according to other authors, the full achievement of reproducibility may require additional components.

@kitzes2017practice present a collection of case studies on reproducibility practices from across the data-intensive sciences, illustrating a variety of recommendations and techniques for achieving reproducibility. Although their work does not come to a consensus on the exact standards of reproducibility that should be followed, several common trends and principles emerge from their case studies that extend beyond the minimum recommendations of @Goodman341ps12: 

1) use clear separation, labeling, and documentation in provided code,
2) automate processes when possible, and
3) design the data analysis workflow as a sequence of small steps glued together, with outputs from one step serving as inputs into the next. This is a common suggestion within the computing community, originating as part of the Unix philosophy (@unix).

@cooper2017guide focus on data analysis completed in `R` and identify a similar list of important reproducibility components, reinforcing the need for clearly labeled, well-documented, and well-separated files. In addition, they recommend publishing a list of software dependencies and using version control to track project changes over time.

@broman reiterates the need for clear naming and file separation while sharing several additional suggestions: keep the project contained in one directory, use relative paths when accessing the file system, and include a `README` file describing the project.

The reproducibility recommendations from R OpenSci, a non-profit initiative founded in 2011 to make scientific data retrieval reproducible, share similar principles to those discussed previously. They focus on a need for a well-developed file system, with no extraneous files and clear labeling. They also reiterate the need to note dependencies and use automation when possible, while making clear a suggestion not present in the previously-discussed literature: the need to use seeds, which allow for the saving and restoring of the random number generator state, when running code involving randomness (@r-opensci).

Although these recommendations differ from one another, when considered in combination they provide a well-rounded picture of the components important to research reproducibility across the scientific community:

1. The basic project components are made accessible:

  - Data (raw and/or processed)
  - Metadata
  - Code
  - Related Software

2. The file structure of project is well-organized:

  - Separate folders for different file types.
  - No extraneous files.
  - Minimal clutter.

3. The project is documented well:

  - Files are clearly named, preferably in a way where the order in which they should be run is clear.
  - A README is present.
  - Software dependencies are noted.

4. File paths used in code are not system- or user-dependent:

  - No absolute paths.
  - No paths leading to locations outside of a project's directory.
  - Only relative paths, pointing to locations within a project's directory, are permitted.

5. Randomness is accounted for:

  - If randomness is used in code, a seed must also be set.

6. Readable, styled code:

  - Though not mentioned in the sources described previously, it is also important that code be written in a coherent style. This is because code that conforms to a style guide or is written in a consistent dialect is easier to read, simplifying the process of following a researcher's work from beginning to end (@hermans2017programming). 

## Current Attempts to Address Reproducibility in Academia

### Journal Requirements

In an attempt to increase reproducibility in the sciences, leaders from academic journals around the world have taken steps to create new standards and requirements for submitted articles. However, these standards are highly inconsistent, varying significantly both across and within disciplines.

The journal whose requirements appear to align most closely with those components defined previously is the *American Journal of Political Science* (AJPS). In 2012, the AJPS became the first political science journal to require authors to make their data openly accessible online, and the publication has instituted stricter requirements since. AJPS now requires that authors submit the following alongside their papers:

* The dataset analyzed in the paper and information about its source. If the dataset has been processed, instructions for manipulating the raw data to achieve the final data must also be shared.
* Detailed, clear code necessary for reproducing all of the tables and figures in the paper.
* Documentation, including a README and codebook.
* Information about the software used to conduct the analysis, including the specific versions and packages used.

**ADD SOURCE:** https://ajps.org/wp-content/uploads/2018/05/ajps_replication-guidelines-2-1.pdf

These standards are quite thorough, meeting a variety of the conditions described previously. 

Most journals, however, do not come close to meeting such high standards in their reproducibility statements. We will consider examples from several fields:

A group of editors reproesenting over 30 major journals from the biomedical sciences met in 2014 to address reproducibility in their field, coming to a consensus on a set of principles they wanted to uphold. Below are those relating specifically to the use of data and statistical methods:

1) The journal should have a mechanism to check the statistical accuracy of submissions.

2) Journals should have no (or generous) limit on methods section length.

3) Journals should use a checklist to ensure the reporting of key information, including: 

  - The article meets nomenclature/reporting standards of the biomedical field
  - Investigators report how often each experiment was performed and whether results were substantiated by repetition under a range of conditions.
  - Statistics must be fully reported in the paper (including test used, value of N, definition of center, dispersion and precision measures).
  - Authors must state whether samples were randomized and how.
  - Authors must state whether the experiment was blinded.
  - Authors must clearly state the criteria used for exclusion of any data or subjects and must include all results, even those that do not support the main findings.
  
4) All datasets must be made available on request and should be on public repositories when possible. If not possible, data values should be presented in the paper or supplementary information.

5) Software sharing should be encouraged. At the minimum, authors should provide a statement describing if software is available and how to obtain it.

**ADD SOURCE:** https://www.nature.com/news/journals-unite-for-reproducibility-1.16259

Even though these principles seem well-developed on the surface, they fail to meet even the basic requirements defined by @Goodman341ps12 previously. There is no requirement that code be shared, nor metadata, and software requirements are somewhat loose.

*Experimental Results*, a journal created by Cambridge University Press for the specific purpose of addressing some of the reproducibility and open access issues in academia, also fails to meet these requirements. Allowing articles from a variety of disciplines, their transparency and openness policy states only that "whenever possible authors should make evidence and resources that underpin published findings, such as data, code, and other materials, available to readers without undue barriers to access." It is only recommended, not required, that code and data be fully available. The reproducibility components that extend beyond what is required at a minimum are not considered.

**ADD SOURCE:** https://www.cambridge.org/core/journals/experimental-results/information/transparency-and-openness-policy


https://www.pnas.org/content/115/11/2584

https://www.researchgate.net/publication/288818031_Reproducible_research_in_statistics_A_review_and_guidelines_for_the_Biometrical_Journal

Guidelines from top Statistics Journals:
https://www.jstatsoft.org/pages/view/authors 

https://www.tandfonline.com/action/authorSubmission?show=instructions&journalCode=uasa20 (code + data)

https://www.tandfonline.com/action/authorSubmission?show=instructions&journalCode=ucgs20 (code + data)

https://www.tandfonline.com/action/authorSubmission?show=instructions&journalCode=utas20 (strongly encourages code + data)

Annals of statistics --- nothing?? https://imstat.org/journals-and-publications/annals-of-statistics/annals-of-statistics-manuscript-submission/

https://journal.r-project.org/share/author-guide.pdf -- code + data







### Reproducibility Education



