# Addressing the Challenges of Reproducibility {#my-solution}

## Review Of Previous Work

### Literature

Publications on reproducibility can be found in all areas of scientific research. However, as @Goodman341ps12 argue, the language and conceptual framework of research reproducibility varies significantly across the sciences, and there are no clear standards on reproducibility agreed upon by the scientific community as a whole. We consider recommendations from a variety of fields and determine the key aspects of reproducibility faced by scientists in different disciplines. 

@kitzes2017practice present a collection of case studies on reproducibility practices from across the data-intensive sciences, illustrating a variety of recommendations and techniques for achieving reproducibility. Although their work does not come to a consensus on the exact standards of reproducibility that should be followed, several common trends and principles emerge from their case studies: 1) use clear separation, labeling, and documentation, 2) automate processes when possible, and 3) design the data analysis workflow as a sequence of small steps glued together, with outputs from one step serving as inputs into the next. This is a common suggestion within the computing community, originating as part of the Unix philosophy (@unix).

@cooper2017guide focus on data analysis in `R` and identify a similar list of important reproducibility components, reinforcing the need for clearly labeled, well-documented, and well-separated files. In addition, they recommend publishing a list of dependencies and using version control. 
@broman reiterates the need for clear naming and file separation while sharing several additional suggestions: keep the project contained in one directory, use relative paths, and include a `README`.

The reproducibility recommendations from R OpenSci, a non-profit initiative founded in 2011 to make scientific data retrieval reproducible, share similar principles to those discussed previously. They focus on a need for a well-developed file system, with no extraneous files and clear labeling. They also reiterate the need to note dependencies and use automation when possible, while making clear a suggestion not present in the previously-discussed literature: the need to use seeds, which allow for the saving and restoring of the random number generator state, when running code involving randomness (@r-opensci).

When considered in combination, these sources provide a well-rounded picture of the components important to research reproducibility. Using this literature as a guideline, we identify several key features of reproducible work. These recommendations are a matter of opinion---due to the lack of agreement on which components of reproducibility are most important, we select those that are mentioned most often, as well as some that are mentioned less but that we view as important. 


1. A well-designed file structure:

  - Separate folders for different file types.
  - No extraneous files.
  - Minimal clutter.

2. Good documentation:

  - Files are clearly named, preferably in a way where the order in which they should be run is clear.
  - A README is present.
  - Dependencies are noted.

3. Reproducible file paths:

  - No absolute paths, or paths leading to locations outside of a project's directory, are used in code---only portable (relative) paths.

4. Randomness is accounted for:

  - If randomness is used in code, a seed must also be set.

5. Readable, styled code:

  - Code should be written in a coherent style. Code that conforms to a style guide or is written in a consistent dialect is easier to read (@hermans2017programming). We believe that the `tidyverse` provides the most accessible dialect of `R`.


Much of the available literature focuses on file structure, organization, and naming, and `fertile`'s features are consistent with this. @marwick2018packaging} provide the framework for file structure that `fertile` is based on: a structure similar to that of an `R` package (@coreteam-extensions, @hadley-packages), with an `R` folder, as well as `data`, `data-raw`, `inst`, and `vignettes`.

### R Packages and Other Software

Much of this work is highly generalized, written to be applicable to users working with a variety of statistical software programs. Because all statistical software programs operate differently, these recommendations are inherently vague and difficult to implement, particularly to new analysts who are relatively unfamiliar with their software. Focused attempts to address reproducibility in specific certain software programs are more likely to be successful. We focus on `R`, due to its open-source nature, accessibility, and popularity as a tool for statistical analysis.

A small body of `R` packages focuses on research reproducibility. `rrtools` (@R-rrtools) addresses some of the issues discussed in @marwick2018packaging by creating a basic `R` package structure for a data analysis project and implementing a basic `testthat::check` functionality. The `orderly` (@R-orderly) package also focuses on file structure, requiring the user to declare a desired project structure (typically a step-by-step structure, where outputs from one step are inputs into the next) at the beginning and then creating the files necessary to achieve that structure. `workflowr`'s (@R-workflowr) functionality is based around version control and making code easily available online. It works to generate a website containing time-stamped, versioned, and documented results. `checkers` (@R-checkers) allows you to create custom checks that examine different aspects of reproducibility. `packrat` (@R-packrat) is focused on dependencies, creating a packaged folder containing a project as well as all of its dependencies so that projects dependent on lesser-used packages can be easily shared across computers. `drake` (@R-drake) analyzes workflows, skips steps where results are up to date, and provides evidence that results match the underlying code and data. Lastly, the `reproducible` (@R-reproducible) package focuses on the concept of caching: saving information so that projects can be run faster each time they are re-completed from the start.

Many of these packages are narrow, with each effectively addressing a small component of reproducibility: file structure, modularization of code, version control, etc. These packages often  succeed in their area of focus, but at the cost of accessibility to a wider audience. Their functions are often quite complex to use, and many steps must be completed to achieve the required reproducibility goal. This cumbersome nature means that most reproducibility packages currently available are not easily accessible to users near the beginning of their `R` journey, nor particularly useful to those looking for quick and easy reproducibility checks.
A more effective way of realizing widespread reproducibility is to make the process for doing so simple enough that it takes little to no conscious effort to implement. You want users to "fall into a hole" (we paraphrase Hadley Wickham) of good practice. 

`Continuous integration` tools provide more general approaches to automated checking, which can enhance reproducibility with minimal code. For example, `wercker`---a command line tool that leverages Docker---enables users to test whether their projects will successfully compile when run on a variety of operating systems without access to the user's local hard drive (@wercker). `GitHub Actions` is integrated into GitHub and can be configured to do similar checks on projects hosted in repositories. `Travis CI` and `Circle CI` are popular continuous integration tools that can also be used to check `R` code. 

However, while these tools can be useful, they are generalized so as to be useful to the widest audience. As a result, their checks are not designed to be `R`-specific, which makes them sub-optimal for users looking to address reproducibility issues involving features specific to the `R`  programming language, such as package installation and seed setting.

## Identifying Gaps In Existing Solutions

## My Contribution: `fertile`, An R Package Creating Optimal Conditions For Reproducibility

### Package Overview

`fertile` attempts to address these gaps in existing software by providing a simple, easy-to-learn reproducibility package that, rather than focusing intensely on a specific area, provides some information about a wide variety of aspects influencing reproducibility. `fertile` is flexible, offering benefits to users at any stage in the data analysis workflow, and provides `R`-specific features, which address certain aspects of reproducibility that can be missed by external project development software.

`fertile` is designed to be used on data analyses organized as `R` Projects (i.e. directories containing an `.Rproj` file). Once an `R` Project is created, `fertile` provides benefits throughout the data analysis process, both during development as well as after the fact. `fertile` achieves this by operating in two modes: proactively (to prevent reproducibility mistakes from happening in the first place), and retroactively (analyzing code that has already been written for potential problems).

### Proactive Use

Proactively, the package identifies potential mistakes as they are made by the user and outputs an informative message as well as a recommended solution. For example, `fertile` catches when a user passes a potentially problematic file path---such as an absolute path, or a path that points to a location outside of the project directory---to a variety of common input/output functions operating on many different file types.

```{r write_csv, include=FALSE}
readr::write_csv(mtcars, "~/Desktop/my_data.csv")
```

```{r fertile, message = FALSE, error = TRUE}
library(fertile)
file.exists("~/Desktop/my_data.csv")
read.csv("~/Desktop/my_data.csv")
read.csv("../../../Desktop/my_data.csv")
```


`fertile` is even more aggressive with functions (like `setwd()`) that are almost certain to break reproducibility, causing them to throw errors that prevent their execution and providing recommendations for better alternatives.

```{r setwd, message=FALSE, error = TRUE}
setwd("~/Desktop")
```


These proactive warning features are activated immediately after attaching the `fertile` package and require no additional effort by the user.

### Retroactive Use

Retroactively, `fertile` analyzes potential obstacles to reproducibility in an RStudio Project (i.e., a directory that contains an `.Rproj` file). The package considers several different aspects of the project which may influence reproducibility, including the directory structure, file paths, and whether randomness is used thoughtfully.

The end products of these analyses are reproducibility reports summarizing a project's adherence to reproducibility standards and recommending remedies for where the project falls short. For example, `fertile` might identify the use of randomness in code and recommend setting a seed if one is not present.

Users can access the majority of `fertile`'s retroactive features through two primary functions, `proj_check()` and `proj_analyze()`. 

The `proj_check()` function runs fifteen different reproducibility tests, noting which ones passed, which ones failed, the reason for failure, a recommended solution, and a guide to where to look for help. These tests include: looking for a clear build chain, checking to make sure the root level of the project is clear of clutter, confirming that there are no files present that are not being directly used by or created by the code, and looking for uses of randomness that do not have a call to `set.seed()` present. A full list is provided below:

```{r list-checks}
list_checks()
```


Subsets of the fifteen tests can be invoked using the `tidyselect` helper functions (@R-tidyselect) in combination with the more limited `proj_check_some()` function.

```{r set-project}
proj_dir <- "project_miceps"
```

```{r echo=FALSE}
bad_file <- fs::path(proj_dir, "install_proj_packages.R")
if (file.exists(bad_file)) {
  fs::file_delete(bad_file)
}

```

```{r proj_check_some, warning = FALSE}
proj_check_some(proj_dir, contains("paths"))
```


Each test can also be run individually by calling the function matching its check name.

The `proj_analyze()` function creates a report documenting the structure of a data analysis project. This report contains information about all packages referenced in code, the files present in the directory and their types, suggestions for moving files to create a more organized structure, and a list of reproducibility-breaking file paths used in code.

```{r proj_analyze, out.width = 180}
proj_analyze(proj_dir)
```


### Logging

`fertile` also contains logging functionality, which records commands run in the console that have the potential to affect reproducibility, enabling users to look at their past history at any time. The package focuses mostly on package loading and file opening, noting which function was used, the path or package it referenced, and the timestamp at which that event happened. Users can access the log recording their commands at any time via the `log_report()` function:

```{r read-csv, message=FALSE, include=FALSE}
library(purrr)
library(forcats)
read_csv(fs::path(proj_dir, "mice.csv"))
```

```{r log-report}
log_report()
```

The log, if not managed, can grow very long over time. For users who do not desire such functionality, `log_clear()` provides a way to erase the log and start over.

```{r log-clear, include=FALSE}
log_clear()
log_report()
```

### Utility Functions

`fertile` also provides several useful utility functions that may assist with the process of data analysis. 

### File Path Management

The `check_path()` function analyzes a vector of paths (or a single path) to determine whether there are any absolute paths or paths that lead outside the project directory.

```{r path checks, error = TRUE, message = FALSE}
# Path inside the directory
check_path("project_miceps")

# Absolute path (current working directory)
check_path(getwd())

# Path outside the directory
check_path("../fertile.Rmd")
```

### File Types

There are several functions that can be used to check the type of a file: 

```{r is_file}
is_data_file(fs::path(proj_dir, "mice.csv"))
is_image_file(fs::path(proj_dir, "proteins_v_time.png"))
is_text_file(fs::path(proj_dir, "README.md"))
is_r_file(fs::path(proj_dir, "analysis.Rmd"))
```

### Temporary Directories

The `sandbox()` function allows the user to make a copy of their project in a temporary directory. This can be useful for ensuring that projects run properly when access to the local file system is removed.

```{r sandbox}
proj_dir
fs::dir_ls(proj_dir) %>% head(3)
temp_dir <- sandbox(proj_dir)
temp_dir
fs::dir_ls(temp_dir) %>% head(3)
```


### Managing Project Dependencies

One of the challenges with ensuring that work is reproducible is the issue of dependencies. Many data analysis projects reference a variety of `R` packages in their code. When such projects are shared with other users who may not have the required packages downloaded, it can cause errors that prevent the project from running properly. 

The `proj_pkg_script()`} function assists with this issue by making it simple and fast to download dependencies. When run on an `R` project directory, the function creates a `.R` script file that contains the code needed to install all of the packages referenced in the project, differentiating between packages located on CRAN and those located on GitHub.

```{r proj_pkg_script, eval=TRUE, cache=TRUE}
install_script <- proj_pkg_script(proj_dir)
cat(readChar(install_script, 1e5))
```


## How `fertile` Works


Much of the functionality in `fertile` is achieved by writing `shims` **link to wikipedia page here**. `fertile`'s shimmed functions intercept the user's commands and perform various logging and checking tasks before executing the desired function. Our process is:

1. Identify an `R` function that is likely to be involved in operations that may break reproducibility. Popular functions associated with only one package (e.g., `read_csv()` from `readr`) are ideal candidates.

2. Create a function in `fertile` with the same name that takes the same arguments (and always the dots `...`).

3. Write this new function so that it: 
  a) captures any arguments, 
  b) logs the name of the function called, 
  c) performs any checks on these arguments, and 
  d) calls the original function with the original arguments. Except where warranted, the execution looks the same to the user as if they were calling the original function.
  
Most shims are quite simple and look something like what is shown below for `read_csv()`. 

```{r}
fertile::read_csv
```


`fertile` shims many common functions, including those that read in a variety of data types, write data, and load packages. This works both proactively and retroactively, as the shimmed functions written in `fertile` are activated both when the user is coding interactively and when a file containing code is rendered.

In order to ensure that the `fertile` versions of functions ("shims") always supersede ("mask") their original namesakes when called, `fertile` uses its own shims of the `library` and `require` functions to  manipulate the `R` `search` path so that it is always located in the first position. In the `fertile` version of `library()`, we detach `fertile` from the search path, load the requested package, and then re-attach `fertile`. This ensures that when a user executes a command, `R` will check `fertile` for a matching function before considering other packages. While it is possible that this shifty behavior could lead to unintended consequences, our goal is to catch a good deal of problems before they become problematic. Users can easily disable `fertile` by detaching it, or not loading it in the first place. 


## `fertile` in Practice: Experimental Results From Smith College Student Use

`fertile` is designed to: 1) be simple enough that users with minimal `R` experience can use the package without issue, 2) increase the reproducibility of work produced by its users, and 3) educate its users on why their work is or is not reproducible and provide guidance on how to address any problems.

To test `fertile`'s effectiveness, we began an initial randomized control trial of the package on an introductory undergraduate data science course at Smith College in Spring 2020 **ADD FOOTNOTE** (This study was approved by Smith College IRB, Protocol #19-032).

The experiment was structured as follows:


1.Students are given a form at the start of the semester asking whether they consent to participate in a study on data science education. In order to successfully consent, they must provide their system username, collected through the command `Sys.getenv("LOGNAME")`. To maintain privacy the results are then transformed into a hexadecimal string via the `md5()` hashing function. 

2. These hexadecimal strings are then randomly assigned into equally sized groups, one experimental group that receives the features of `fertile` and one group that receives a control.

3. The students are then asked to download a package called `sds192` (the course number and prefix), which was created for the purpose of this trial. It leverages an `.onAttach()` function to scan the `R` environment and collect the username of the user who is loading the package and run it through the same hashing algorithm as used previously. It then identifies whether that user belongs to the experimental or the control group. Depending on the group they are in, they receive a different version of the package.

4. The experimental group receives the basic `sds192` package, which consists of some data sets and `R` Markdown templates necessary for completing homework assignments and projects in the class, but also has `fertile` installed and loaded silently in the background. The package's proactive features are enabled, and therefore users will receive warning messages when they use absolute or non-portable paths or attempt to change their working directory. The control group receives only the basic `sds192` package, including its data sets and `R` Markdown templates. All students from both groups then use their version of the package throughout the semester on a variety of projects.

5. Both groups are given a short quiz on different components of reproducibility that are intended to be taught by `fertile` at both the beginning and end of the semester. Their scores are then compared to see whether one group learned more than the other group or whether their scores were essentially equivalent. Additionally, for every homework assignment submitted, the professor takes note of whether or not the project compiles successfully.


Based on the results, we hope to determine whether `fertile` was successful at achieving its intended goals. A lack of notable difference between the *experimental* and *control* groups in terms of the number of code-related questions asked throughout the semester would indicate that `fertile` achieved its goal of simplicity. A higher average for the *experimental* group in terms of the number of homework assignments that compiled successfully would indicate that `fertile` was successful in increasing reproducibility. A greater increase over the semester in the reproducibility quiz scores for students in the *experimental* group compared with the *control* group would indicate that `fertile` achieved its goal of educating users on reproducibility. Success according to these metrics would provide evidence showing `fertile`'s benefit as tool to help educators introduce reproducibility concepts in the classroom.

