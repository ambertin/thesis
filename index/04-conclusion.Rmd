# Conclusion {-}

In the field of data science, research is considered fully *reproducible* when the requisite code and data files produce identical results when run by another analyst.

Reproducibility is incredibly important to the scientific community, helping ensure the accuracy of the findings of studies and data analyses and simplifying the process of collaboration and knowledge sharing. When all of the files necessary for a study to be run are published simultaneously, it makes it much easier for others to understand the methods and ideas that were used and apply them to other work in similar areas, promoting the process of scientific advancement. 

However, even though it has many benefits in the scientific community, reproducibility is currently facing a crisis. The vast majority of researchers across all scientific fields have been unable to reproduce another researcher's results, and many have been unable to reproduce even their own. In some fields, more than half of published articles have failed attempts at reproducibility.

Recognizing the issue, scientists from a variety of communities have begun to write about reproducibility, working to define and spread information about the components necessary for a project to be considered reproducible. Though there has not been a clear consensus on the exact components necessary, several clear trends emerge when looking at a variety of authors' reproducibility criteria. These, when combined, require that a reproducible project meet the following standards.

1. The basic project components are made accessible to the public.

2. The file structure of project is well-organized.

3. The project is documented well.

4. File paths used in code are not system- or user-dependent.

5. Randomness is accounted for.

6. Code is readable and consistently styled.


