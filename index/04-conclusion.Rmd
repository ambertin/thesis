# Conclusion {-}

In the field of data science, research is considered fully *reproducible* when the requisite code and data files produce identical results when run by another analyst. Though there is no clear consensus on the exact standards necessary to achieve reproducibility, writing on the topic identifies several components of great importance:

1. The basic project components are made accessible to the public.

2. The file structure of project is well-organized.

3. The project is documented well.

4. File paths used in code are not system- or user-dependent.

5. Randomness is accounted for.

6. Code is readable and consistently styled.

Reproducibility is incredibly important to the scientific community, helping ensure the accuracy of the findings of studies and data analyses and simplifying the process of collaboration and knowledge sharing. When all of the files necessary for a study to be run are published simultaneously, it makes it much easier for others to understand the methods and ideas that were used and apply them to other work in similar areas, promoting the process of scientific advancement. 

However, even though it has many benefits in the scientific community, reproducibility is currently facing a crisis. The vast majority of researchers across all scientific fields have been unable to reproduce another researcher's results, and many have been unable to reproduce even their own. In some fields, more than half of published articles have failed attempts at reproducibility.

Researchers across the sciences have recognized this problem and taken steps to address it. Data-intensive academic journals have put in place reproducibility guidelines requiring that submitted articles meet at least some of the standards listed above. Software developers have built tools to help data analysts make their projects more reproducible. These include a small library of `R` packages, which provide benefits to users of the popular statistical language `R`, as well as several continuous integration tools, which are much more broad in their application. Educators have also taken action on reproducibility, introducing courses and workshops focused on the topic at their universities. 

However, many of these solutions have fallen short. Even with reproducibility guidelines, many journals still publish work that is not reproducible. Journal review takes significantly longer and requires many more resources when ensuring that reproducibility guidelines are fully met. Journals often cannot spare the additional time and money, leaving it up to submitting authors to ensure that their articles meet standards. Since the adherence to standards is not often checked, and achieving full reproducibility can sometimes be time consuming and challenging, many authors fail to take this step. 

Software solutions have not fared much better. Many of the `R` specific solutions only address a single component of reproducibility---for example, version control---and leave out the rest, making it challenging for researchers to address their problems all at once. Additionally, a lot of the software is quite complex in its operation, containing barriers to entry that prevent less-experienced `R` users from trying them. The continuous integration tools do not face the same issues, but have their own problems. Due to their general nature, these tools do not have any way of addressing coding-language-specific reproducibility problems such as managing software dependencies.

Finally, educational attempts to address reproducibility are inaccessible and time consuming to implement. Many of the existing educational options require a great deal of research knowledge, and as a result are only available at the graduate level, leaving undergraduate students with little to no exposure on the subject. Additionally, many current versions of reproducibility education require a lot of time and focus in class. Professors wanting to share the topic of reproducibility with their students must do so while sacrificing a significant portion of time from other important topics. 

Due to their many associated shortcomings, none of these solutions appear to have the potential to address the problem of reproducibility on a wider scale. That is where my work comes in. I have worked to develop `fertile`, a package built for `R` users that is focused on creating optimum conditions for reproducibility for data analysis projects written in `R`.

The package is designed to possess none of the challenges apparent in other solutions---to serve as a one-stop solution, where users can go to address all of their reproducibility needs at one time. Rather than being time consuming, complicated, inaccessible, or too focused on one issue, `fertile` is:

1) *Simple, with a small library of functions/tools that are straightforward to use*. Some of the package's functionalities, like the interactive file path warning system, require no effort from the user to enable, while most others can be run with only a handful of functions requiring one argument: a path to a project directory.

2) *Accessible to a variety of users, with a relatively small learning curve.* Users do not need a background in research, nor do they need an advanced knowledge of `R` programming, to run the majority of functions in `fertile`.

3) *Able to address a wide variety of aspects of reproducibility, rather than just one or two key issues.* `fertile` contains functions to address all six previously defined reproducibility components, unlike most software which addresses only one or two.

4) *Language specific, possessing features that address some of the reproducibility challenges associated with `R`.* The `has_no_randomness()`, `proj_pkg_script()`, and `proj_dependency_report()` functions handle the `R`-specific issues of random number generation and package dependencies.

5) *Customizable, allowing users to choose for themselves which aspects of reproducibility they want to focus on.* There is no one way that users are forced to use `fertile`. Those who only want to focus on one aspect of reproducibility can do so, while those who want to address everything at once have that option as well. Users can also control which functions interactively warn them about non-reproducible file paths.

6) *Educational, teaching those that use it about why their projects are not reproducible and how to correct that in the future.* Throughout the reproducibility checking process, `fertile` provides informative messages that explain to users the flaws in their code and show them ways to correct the issues.

Due to its many advantages compared with traditional reproducibility solutions, `fertile` has the potential to provide significant benefits in a variety of domains, including in the areas of journal review and education, where other solutions have not quite met the mark.

In the journal review process, `fertile` can be integrated to help ensure that submitted articles meet reproducibility standards. Since `fertile` is both free and publicly available, authors and journal reviewers could take advantage of it in their reproducibility-checking process. Journals might require that certain `fertile` checks be passed for articles to be approved, submitting authors could ensure that their projects pass on their end, and then all that would be required for journals to ensure reproducibility would be a quick run of `fertile`'s functions, or to require that authors submit a `proj_badges()` report certifying the project's adherence to reproducibility standards. This would greatly reduce the barriers on both the reviewing and authoring side of the issue and would vastly speed up the reproducibility-checking process, making it much easier to ensure that published articles are truly reproducible.

In the classroom, `fertile` could be used to extend reproducibility education to undergraduate students---even those at the introductory level. Due to its simplicity, the package would take minimal time and effort to integrate into courses, allowing professors to teach reproducibility without taking important time away from other key topics. If professors were to integrate `fertile` in to their courses, students would be able begin learning about reproducibility much earlier in their data science careers than would likely happen otherwise, increasing the chance that they prioritize reproducibility in their future work.

Academia and research are not the only applications, however. `fertile` could also assist the general world of data science. `R` bloggers and Tidy Tuesday participants could use the package to improve the quality of their code- and data- sharing, making it easier for others to learn from their work. Data journalists, in some situations, could do something similar, sharing the (reproducible) code and data behind their projects to remove the black-box nature that some such projects possess, building a relationship of trust with their readers. And private companies doing consulting work could integrate `fertile` into their workflow as a way of increasing transparency between themselves and their clients. 

To test `fertile's` effectiveness in the classroom, we designed a randomized experiment on an introductory data science course to compare reproducibility learning between students who received `fertile` on their computer and those who didn't. Although no indication of a relationship between `fertile` use and knowledge improvement was found in this study—with the caveat that the study format, with blinding, did not fit the intended use of `fertile` in the real world—the experiment design itself has the potential to open up new avenues of code and package testing in the `R` community. 

`R` programmers working in a variety of domains could use the experimental structure to employ A/B testing of their software, a type of scientifically-backed testing that has previously not been used—at least to a notable degree—to test `R` code. This method of testing could pave the way for developers to scientifically measure which coding practices are most effective, efficient, and user-friendly.

Additionally, outside of the experiment, `fertile` appears to already be making an impact in the `R` community. A paper on the package, written by myself and Professor Baumer, was published in the journal `Stat` in early 2021 and the project has also gained traction on social media, with many influential `R` community members sharing it to their networks. 

Although it is difficult to predict the future of `fertile`, my hope is for this project is for it to help bring the `R` community, even if just a few users, toward improved reproducibility and to improve knowledge around the issue within the community. Anything beyond that—journal integration of `fertile` features, deployment of `fertile`'s A/B testing design, integration into the workflow by RStudio employees, or the like—would just be the cherry on top.