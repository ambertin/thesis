# Conclusion {-}

In the field of data science, research is considered fully *reproducible* when the requisite code and data files produce identical results when run by another analyst. Though there is no clear consensus on the exact standards necessary to achieve reproducibility, we identify several components of importance:

1. The basic project components are made accessible to the public.

2. The file structure of project is well-organized.

3. The project is documented well.

4. File paths used in code are not system- or user-dependent.

5. Randomness is controlled.

6. Code is readable and consistently styled.

Reproducibility is vital to the scientific community, helping ensure the accuracy of the findings of studies and data analyses and simplifying the process of collaboration and knowledge sharing. When all of the files necessary for a study to be run are published simultaneously, it makes it much easier for others to understand the methods and ideas that were used and apply them to other work in similar areas, promoting the process of scientific advancement. 

However, even though it has many benefits in the scientific community, reproducibility is currently facing a crisis. Many researchers across all scientific fields have been unable to reproduce each other's results, and some have been unable to reproduce even their own. In some fields, more than half of published articles have failed attempts at reproducibility.

Researchers across the sciences have recognized this problem and taken steps to address it. Data-intensive academic journals have put in place reproducibility guidelines requiring that submitted articles meet at least some of the standards listed above. Software developers have built tools to help data analysts make their projects more reproducible. These include a small library of `R` packages, which provide benefits to users of the popular statistical language `R`, as well as several continuous integration tools, which are much more broad in their application. Educators have also taken action on reproducibility, introducing courses and workshops focused on the topic at their universities. However, many of these solutions are sub-optimal, facing challenges with inaccessibility, steep learning curves, limited functionality, and/or lack of coding language-specific features.

`fertile` attempts to fill this gap by being:

1) Simple, with a small library of functions/tools that are straightforward to use. 

2) Accessible to a variety of users, with a relatively small learning curve. 

3) Able to address a wide variety of aspects of reproducibility, rather than just one or two key issues.

4) Language specific, possessing features that address some of the reproducibility challenges associated with `R`.

5) Customizable, allowing users to choose for themselves which aspects of reproducibility they want to focus on.

6) Educational, teaching those that use it about why their projects are not reproducible and how to correct that in the future.

Due to its many advantages compared with traditional reproducibility solutions, `fertile` has the potential to provide  benefits in a variety of domains, including in the areas of journal review and education, where other solutions have not quite met the mark.

In the journal review process, `fertile` can be integrated to help ensure that submitted articles meet reproducibility standards. Since `fertile` is both free and publicly available, authors and journal reviewers could take advantage of it in their reproducibility-checking process. Journals might require that certain `fertile` checks be passed for articles to be approved, submitting authors could ensure that their projects pass on their end, and then all that would be required for journals to ensure reproducibility would be a quick run of `fertile`'s functions, or to require that authors submit a `proj_badges()` report certifying the project's adherence to reproducibility standards. This would greatly reduce the barriers on both the reviewing and authoring side of the issue and would vastly speed up the reproducibility-checking process, making it much easier to ensure that published articles are truly reproducible.

In the classroom, `fertile` could be used to extend reproducibility education to undergraduate students---even those at the introductory level. Due to its simplicity, the package would take minimal time and effort to integrate into courses, allowing professors to teach reproducibility without taking important time away from other key topics. If professors were to integrate `fertile` in to their courses, students would be able begin learning about reproducibility much earlier in their data science careers than would likely happen otherwise, increasing the chance that they prioritize reproducibility in their future work.

Academia and research are not the only applications, however. `fertile` could also assist the general world of data science. `R` bloggers and Tidy Tuesday participants could use the package to improve the quality of their code- and data- sharing, making it easier for others to learn from their work. Data journalists, in some situations, could do something similar, sharing the (reproducible) code and data behind their projects to remove the black-box nature that some such projects possess, building a relationship of trust with their readers. And private companies doing consulting work could integrate `fertile` into their workflow as a way of increasing transparency between themselves and their clients. 

To test `fertile's` effectiveness in the classroom, we designed a randomized experiment on an introductory data science course to compare reproducibility learning between students who received `fertile` on their computer and those who didn't. Although no indication of a relationship between `fertile` use and knowledge improvement was found in this study—with the caveat that the study format, with blinding, did not fit the intended use of `fertile` in the real world—the experiment design itself has the potential to open up new avenues of code and package testing in the `R` community. 

`R` programmers working in a variety of domains could use the experimental structure to employ A/B testing of their software, a type of scientifically-backed testing that has previously not been used—at least to a notable degree—to test `R` code. This method of testing could pave the way for developers to scientifically measure which coding practices are most effective, efficient, and user-friendly.

Although it is difficult to predict the future of `fertile`, my hope is for this project is for it to help bring the `R` community, even if just a few users, toward improved reproducibility and to improve knowledge around the issue within the community. Anything beyond that—journal integration of `fertile` features, deployment of `fertile`'s A/B testing design, integration into the workflow by RStudio employees, or the like—would just be the cherry on top.