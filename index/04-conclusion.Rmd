# Conclusion {-}

In the field of data science, research is considered fully *reproducible* when the requisite code and data files produce identical results when run by another analyst. Though there is no clear consensus on the exact standards necessary to achieve reproducibility, we identify several components of importance:

1. The basic project components are made accessible to the public.

2. The file structure of project is well-organized.

3. The project is documented well.

4. File paths used in code are not system- or user-dependent.

5. Randomness is controlled.

6. Code is readable and consistently styled.

Reproducibility is vital to the scientific community, helping ensure the accuracy of the findings of studies and data analyses and simplifying the process of collaboration and knowledge sharing. When all of the files necessary for a study to be run are published simultaneously, it makes it much easier for others to understand the methods and ideas that were used and apply them to other work in similar areas, promoting the process of scientific advancement. 

However, even though it has many benefits in the scientific community, reproducibility is currently facing a crisis. Many researchers across all scientific fields have been unable to reproduce each other's results, and some have been unable to reproduce even their own. In some fields, more than half of published articles have failed attempts at reproducibility.

Researchers across the sciences have recognized this problem and taken steps to address it. Data-intensive academic journals have put in place reproducibility guidelines requiring that submitted articles meet at least some of the standards listed above. Software developers have built tools to help data analysts make their projects more reproducible. These include a small library of `R` packages, which provide benefits to users of the popular statistical language `R`, as well as several continuous integration tools, which are much more broad in their application. Educators have also taken action on reproducibility, introducing courses and workshops focused on the topic at their universities. However, many of these solutions are sub-optimal, facing challenges with inaccessibility, steep learning curves, limited functionality, and/or lack of coding language-specific features.

`fertile` attempts to fill this gap by being:

1) Simple, with a small library of functions/tools that are straightforward to use. 

2) Accessible to a variety of users, with a relatively small learning curve. 

3) Able to address a wide variety of aspects of reproducibility, rather than just one or two key issues.

4) Language specific, possessing features that address some of the reproducibility challenges associated with `R`.

5) Customizable, allowing users to choose for themselves which aspects of reproducibility they want to focus on.

6) Educational, teaching those that use it about why their projects are not reproducible and how to correct that in the future.

Due to its many advantages compared with traditional reproducibility solutions, `fertile` has the potential to provide  benefits in a variety of domains, including in the areas of journal review and education, where other solutions have not quite met the mark.

Integration of `fertile` into the journal review process---through requirements for passing `fertile` checks or the inclusion of a `proj_badges()` report with a manuscript submission---would greatly reduce the barriers to reproducibility review, speeding up the review process and making it much easier to ensure that published articles are truly reproducible. 

In the classroom, `fertile` could be used to extend reproducibility education to undergraduate students---even those at the introductory level, introducing students to reproducibility much earlier in their data science careers than would likely happen otherwise and increasing the chance that they prioritize reproducibility in their future work.

 `fertile` could also assist the general world of data science, through inclusion of popular events like Tidy Tuesday or integration into workplace environments, among other possibilities.
 
To test `fertile's` effectiveness in the classroom, we designed a randomized experiment on an introductory data science course to compare reproducibility learning between students who received `fertile` on their computer and those who didn't. Although no indication of a relationship between `fertile` use and knowledge improvement was found in this study—with the caveat that the study format, with blinding, did not fit the intended use of `fertile` in the real world—the experiment design itself has the potential to open up new avenues of code and package testing in the `R` community. 

`R` programmers working in a variety of domains could use the experimental structure to employ A/B testing of their software, a type of scientifically-backed testing that has previously not been used—at least to a notable degree—to test `R` code. This method of testing could pave the way for developers to scientifically measure which coding practices are most effective, efficient, and user-friendly.

Although it is difficult to predict the future of `fertile`, my hope is for this project is for it to help bring the `R` community, even if just a few users, toward improved reproducibility and to improve knowledge around the issue within the community. Anything beyond that—journal integration of `fertile` features, deployment of `fertile`'s A/B testing design, integration into the workflow by RStudio employees, or the like—would just be the cherry on top.

#### A Note on Limitations

Although this work focuses primarily on the advantages of `fertile`, it is important to recognize that `fertile` is not a be-all, end-all solution to reproducibility. `fertile` has its own set of limitations, including the following:

- Its work with dependency-management is far less advanced than some other software. Other packages like `packrat`, for example, go even further, allowing the user to build files containing exact copies of their dependency environment, and add/remove packages as desired.

- `fertile` has only been under development for a handful of years, much of which was part time by only one or two people as a side project. There are certainly features that would be helpful to be included in the package that have not been, simply because there were only so many things that could be reasonably achieved in the time the package was being written.

- `fertile` has around 30 software dependencies. This makes it highly susceptible to functionality-breaking changes if some of the functions `fertile` is dependent on are updated in a way that affects their operation. Unfortunately, this is one of the many trade-offs with `fertile`: in order to have a library of functions which address all different aspects of reproducibility, a variety of tools are needed, necessitating the use of many imports. This has been a challenge at several points in `fertile`'s history, requiring the rewriting of functions to address new bugs due to dependency updates. Though not as significant an issue during more recent months, it is likely that more problematic dependency updates will be released in the future.

- As described in Chapter 2, `fertile` could also be considered a sort of malware, executing operations without the user's knowledge and creating hidden files on their computer. This could be undesirable to some users.

For the last two reasons, `fertile` will likely never be available on CRAN, the official `R` package hosting site. Its malware-like behavior and high number of dependencies make the package potentially unstable and intrusive, features which are undesirable for packages. This makes `fertile` slightly less accessible to users who are unfamiliar with the process of installing packages from GitHub or that do not like doing so.

This thesis itself, too, has some limitations. It does not discuss every potential alternative to `fertile`, many of which have advantages over `fertile` that should be recognized. It also does not go into detail about every choice made during package development, which could leave out information of interest. Additionally, much of the work in Chapter 3, on potential applications is purely theoretical---there is no evidence of how `fertile` might actually work in a journal environment or in a classroom under non-experimental conditions. Claims about likely effectiveness are purely based on functionality, and would require much more testing and wide-spread deployment to be backed-up.

Even with all of these limitations, I believe that `fertile` has a lot of power to do good in the field of reproducibility. If it manages to help even just a few individuals by improving the reproducibility of their work, that is a success.

